{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, LSTM, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/Sekhar/OneDrive/INSOFE/20170923-batch29-cse7321c-cute04-team12-master/data_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Specify these metrics before hand. Gonna be useful later on.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Write a function to clean the conversations\n",
    "def clean_converse(raw_converse):\n",
    "    #Convert the words to lower and split\n",
    "    words = raw_converse.lower().split()\n",
    "    #Stopwords\n",
    "    stop = set(stopwords.words('english'))\n",
    "    #Remove stop words\n",
    "    words = [w for w in words if w not in stop]\n",
    "    #Get the cleaned review\n",
    "    return(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Append the cleaned reviews in the texts list and the target variable in the labels list\n",
    "texts = []\n",
    "labels = []\n",
    "for idx in range(data.converse.shape[0]):\n",
    "    texts.append(clean_converse(data.converse[idx].encode('ascii','ignore')))\n",
    "    labels.append(data.categories[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Keras has some nice pre-processing steps for all the text. Below I'm tokenize the words and converting them to sequences\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39147 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "##Same as above, getting the word indexes. Will be useful later on\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Padding all the sequences to be of equal length\n",
    "data_pad = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Okay, there is a peculiar reason why I'm converting the data into categorical here. \"to_categorical\" from sklearn is not working\n",
    "##straight with the strings in the target variable. So I'm doing this.\n",
    "labels = pd.Categorical(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 1, ..., 4, 4, 4], dtype=int8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Labels.codes gives you the numerical presentations which will be easy to one-hot vector\n",
    "labels.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Create one-hot vectors\n",
    "labels = to_categorical(np.asarray(labels.codes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57244"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Split the data into train and test\n",
    "x_train,x_test,y_train,y_test = train_test_split(data_pad,labels,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 500, 32)       640000      embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 500, 32)       0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 100)           53200       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 100)           0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 6)             606         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 693,806\n",
      "Trainable params: 693,806\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##This is the architecture for RNN, copied straight from the assignment.\n",
    "embedding_vector_length = 32\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(Embedding(MAX_NB_WORDS, embedding_vector_length, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model_LSTM.add(Dropout(0.2))\n",
    "model_LSTM.add(LSTM(100))\n",
    "model_LSTM.add(Dropout(0.2))\n",
    "model_LSTM.add(Dense(6, activation='sigmoid'))\n",
    "model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['recall'])\n",
    "print(model_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45795 samples, validate on 11449 samples\n",
      "Epoch 1/5\n",
      " 2048/45795 [>.............................] - ETA: 2192s - loss: 1.7435 - recall: 0.6660"
     ]
    }
   ],
   "source": [
    "##Fit the model, damn this is taking a lot of time.\n",
    "model_LSTM.fit(x_train, y_train, validation_data=(x_test,y_test), nb_epoch=5, batch_size=256)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.04%\n"
     ]
    }
   ],
   "source": [
    "##This is the accuracy without dropout layers, epochs = 5 and batch size = 128. Highest train accuracy is 84.4%    \n",
    "scores = model_LSTM.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Recall: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
